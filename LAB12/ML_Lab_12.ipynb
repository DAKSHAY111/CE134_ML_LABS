{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN14xaqz10I85ThrYtTn9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAKSHAY111/CE134_ML_LABS/blob/main/LAB12/ML_Lab_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qeuKv7mwoiV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset from torchvision.datasets\n",
        "mnist = MNIST(root='data/', train=True, download=True)\n",
        "\n",
        "# Convert features to float32 and targets to long\n",
        "features_train = mnist.data.float()\n",
        "targets_train = mnist.targets.long()\n",
        "\n",
        "# Normalize features from [0, 255] to [0, 1]\n",
        "features_train /= 255\n",
        "\n",
        "# Flatten features to 1-D vector of 784 features\n",
        "features_train = features_train.view(-1, 784)\n",
        "\n",
        "n_iters = 5000\n",
        "batch_size = 100\n",
        "num_epochs = n_iters // (len(features_train) // batch_size)\n",
        "\n",
        "# Create PyTorch tensor and variable for features and targets for training set\n",
        "train_set = TensorDataset(features_train, targets_train)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "features_train = torch.tensor(features_train, requires_grad=True)\n",
        "targets_train = torch.tensor(targets_train)\n",
        "\n",
        "mnist_test = MNIST(root='data/', train=False, download=True)\n",
        "features_test = mnist_test.data.float()\n",
        "targets_test = mnist_test.targets.long()\n",
        "features_test /= 255\n",
        "features_test = features_test.view(-1, 784)\n",
        "\n",
        "# Create PyTorch tensor for features and targets for test set\n",
        "test_set = TensorDataset(features_test, targets_test)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Visualize one of the images in the training set\n",
        "\n",
        "plt.imshow(mnist.data[2], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ANNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# instantiate ANN\n",
        "input_dim = 28*28\n",
        "\n",
        "# this defines no. of hidden layers\n",
        "# hidden_dim = 150  \n",
        "hidden_dim = 200\n",
        "output_dim = 10\n",
        "\n",
        "# Create ANN\n",
        "model = ANNModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Cross Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.02\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Set number of epochs\n",
        "num_epochs = 5\n",
        "count = 0\n",
        "\n",
        "# Define empty lists to store loss, iteration and accuracy values\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # Convert input and labels to Variables\n",
        "        train = Variable(images.view(-1, 28*28))\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "\n",
        "        # Calculate softmax and cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Count iterations\n",
        "        count += 1\n",
        "\n",
        "        # Calculate accuracy every 50 iterations\n",
        "        if count % 50 == 0:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in test_loader:\n",
        "                test = Variable(images.view(-1, 28*28))\n",
        "                outputs = model(test)\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / float(total)\n",
        "\n",
        "            # Store loss and iteration values\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        # Print loss and accuracy every 500 iterations\n",
        "        if count % 500 == 0:\n",
        "            print('Iteration: {} Loss: {:.4f} Accuracy: {:.2f} %'.\n",
        "                  format(count, loss.data, accuracy))\n"
      ]
    }
  ]
}